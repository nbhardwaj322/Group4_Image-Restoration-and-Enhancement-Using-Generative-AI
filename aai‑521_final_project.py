# -*- coding: utf-8 -*-
"""AAI‑521 Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Te5EAEpUXhoiQzUasX3H31yQhSzVWg81

# AAI‑521 Final Project – Image Restoration and Enhancement with Generative AI

## 1. Introduction

Old or degraded photographs often suffer from noise, low resolution, missing regions, and lack of color.  
This project builds a unified **image restoration and enhancement system** using **pre‑trained generative models from Hugging Face**. The system focuses on four core tasks:

1. **Image Denoising** – remove sensor or compression noise while preserving edges and textures.  
2. **Image Super‑Resolution** – upscale low‑resolution images to higher resolution with sharper details.  
3. **Image Colorization** – add plausible colors to black‑and‑white or grayscale images.  
4. **Image Inpainting** – fill in missing or damaged parts of an image seamlessly.

Instead of training large models from scratch, the project leverages **transfer learning and pretrained diffusion / transformer models** exposed through the diffusers and transformers libraries on Hugging Face. In a full production system, separate training and validation sets would be built from datasets such as ImageNet or COCO by synthetically degrading clean images (adding noise, down‑sampling, converting to grayscale, and masking). In this notebook, the focus is to:

- Demonstrate each enhancement task using strong pretrained models.  
- Show how these models can be fine‑tuned (conceptually and via example code stubs).  
- Integrate all tasks into a **single interactive workflow** so a user can upload an image and choose the desired enhancement.

This project illustrates how modern generative AI models can be combined to form a practical image restoration pipeline suitable for real‑world applications such as photo archives, film restoration, or digital forensics.
"""

!pip install -q diffusers transformers accelerate safetensors gradio==4.44.0 opencv-python-headless==4.10.0.84

import torch
from diffusers import (
    StableDiffusionInpaintPipeline,
    StableDiffusionPipeline
)
from transformers import pipeline
import gradio as gr
import numpy as np
import cv2
from PIL import Image
import matplotlib.pyplot as plt

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

# Utility: ensure PIL image in RGB
def to_rgb(img: Image.Image) -> Image.Image:
    if img.mode != "RGB":
        return img.convert("RGB")
    return img

def pil_to_cv2(img: Image.Image):
    """Convert PIL RGB to OpenCV BGR."""
    arr = np.array(img)
    return cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)

def cv2_to_pil(img_bgr: np.ndarray) -> Image.Image:
    """Convert OpenCV BGR to PIL RGB."""
    rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    return Image.fromarray(rgb)

def show_side_by_side(before: Image.Image, after: Image.Image, title1="Input", title2="Output"):
    """Quick visualization helper (optional for report screenshots)."""
    plt.figure(figsize=(8,4))
    plt.subplot(1,2,1)
    plt.title(title1)
    plt.imshow(before)
    plt.axis("off")
    plt.subplot(1,2,2)
    plt.title(title2)
    plt.imshow(after)
    plt.axis("off")
    plt.show()

"""Denoising model

For denoising, we will simulate noisy images by adding Gaussian noise, then remove it using a strong pretrained diffusion model (runwayml/stable-diffusion-v1-5) in an image‑to‑image configuration (prompt: “clean photo”). This is not a classical BM3D denoiser, but a generative denoiser.
"""

# ===============================================
# Image Denoising with Stable Diffusion (img2img)
# ===============================================

from diffusers import StableDiffusionImg2ImgPipeline

denoise_model_id = "runwayml/stable-diffusion-v1-5"

denoise_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
    denoise_model_id,
    torch_dtype=torch.float16 if device=="cuda" else torch.float32
).to(device)


def add_gaussian_noise(pil_img: Image.Image, sigma: float = 25.0) -> Image.Image:
    """
    Add Gaussian noise (std in [0,255] scale) to simulate a noisy image.
    """
    img = np.array(pil_img).astype(np.float32)
    noise = np.random.normal(0, sigma, img.shape).astype(np.float32)
    noisy = np.clip(img + noise, 0, 255).astype(np.uint8)
    return Image.fromarray(noisy)


def denoise_image(pil_img: Image.Image, strength: float = 0.4, guidance_scale: float = 7.5) -> Image.Image:
    """
    Use Stable Diffusion img2img as a denoiser with prompt 'clean photo'.
    strength: how strongly the model is allowed to modify the image.
    """
    pil_img = to_rgb(pil_img)
    result = denoise_pipe(
        prompt="clean detailed realistic photo, no noise",
        image=pil_img,
        strength=strength,
        guidance_scale=guidance_scale,
        num_inference_steps=30,
    ).images[0]
    return result

# ===============================================
# Image Super-Resolution with diffusers (4x SR)
# ===============================================

from diffusers import StableDiffusionUpscalePipeline

# 4x upscaler model from Hugging Face
sr_model_id = "stabilityai/stable-diffusion-x4-upscaler"

sr_pipe = StableDiffusionUpscalePipeline.from_pretrained(
    sr_model_id,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
).to(device)

def super_resolve(pil_img: Image.Image) -> Image.Image:
    """
    Apply 4x super‑resolution using Stable Diffusion Upscaler.
    The model expects a reasonably small low‑res input (e.g., <= 256x256).
    """
    pil_img = to_rgb(pil_img)

    # Optionally downscale first to simulate a low‑res input
    max_side = 256
    w, h = pil_img.size
    scale = min(max_side / max(w, h), 1.0)
    if scale < 1.0:
        pil_img = pil_img.resize((int(w * scale), int(h * scale)), Image.BICUBIC)

    # Run the upscaler with a generic prompt
    result = sr_pipe(
        prompt="high quality detailed photo",
        image=pil_img,
        num_inference_steps=40,
        guidance_scale=0.0,   # 0 => rely mainly on the input image
    ).images[0]

    return result

# ===============================================
# Image Colorization with Stable Diffusion
# ===============================================

color_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
    denoise_model_id,  # reuse SD v1-5
    torch_dtype=torch.float16 if device=="cuda" else torch.float32
).to(device)

def to_grayscale(pil_img: Image.Image) -> Image.Image:
    return pil_img.convert("L").convert("RGB")  # 3‑channel grayscale

def colorize_image(gray_pil: Image.Image, strength: float = 0.65, guidance_scale: float = 8.0) -> Image.Image:
    """
    Colorize a grayscale image by using SD img2img with a 'color photo' prompt.
    """
    gray_pil = to_rgb(gray_pil)
    result = color_pipe(
        prompt="vibrant realistic color photograph",
        image=gray_pil,
        strength=strength,
        guidance_scale=guidance_scale,
        num_inference_steps=40,
    ).images[0]
    return result

# ===============================================
# Image Inpainting with Stable Diffusion Inpaint
# ===============================================

inpaint_model_id = "runwayml/stable-diffusion-inpainting"

inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(
    inpaint_model_id,
    torch_dtype=torch.float16 if device=="cuda" else torch.float32
).to(device)


def inpaint_image(pil_img: Image.Image, mask_pil: Image.Image,
                  prompt: str = "restore damaged photograph, realistic, high quality",
                  strength: float = 0.75, guidance_scale: float = 7.5) -> Image.Image:
    """
    Inpaint masked regions of the input image.
    - pil_img: original RGB image.
    - mask_pil: white (255) where we want to inpaint, black (0) elsewhere.
    """
    pil_img = to_rgb(pil_img)
    mask_pil = mask_pil.convert("L")  # expected as single‑channel
    result = inpaint_pipe(
        prompt=prompt,
        image=pil_img,
        mask_image=mask_pil,
        strength=strength,
        guidance_scale=guidance_scale,
        num_inference_steps=40,
    ).images[0]
    return result

"""

Upload an image

Choose task (denoise / super‑resolve / colorize / inpaint)

Optionally upload a mask for inpainting

Get enhanced output"""

# ===============================================
# Simple unified interface
# ===============================================
# Assumes:
# - add_gaussian_noise, denoise_image, super_resolve,
#   to_grayscale, colorize_image, inpaint_image, show_side_by_side
#   are already defined.
# ===============================================

from google.colab import files
from PIL import Image
import numpy as np

def run_task_on_uploaded_image(task="Denoising", use_mask=False):
    """
    1. Upload input image (and optional mask if inpainting).
    2. Run the chosen enhancement task.
    3. Show before/after images.

    task: one of ["Denoising", "Super-Resolution", "Colorization", "Inpainting"]
    use_mask: if True and task == "Inpainting", ask for a mask upload.
    """
    print("Please upload the input image file...")
    uploaded = files.upload()
    if not uploaded:
        print("No image uploaded.")
        return

    img_name = list(uploaded.keys())[0]
    input_img = Image.open(img_name).convert("RGB")

    mask_img = None
    if task == "Inpainting" and use_mask:
        print("Please upload a mask image (white = fill region)...")
        uploaded_mask = files.upload()
        if uploaded_mask:
            mask_name = list(uploaded_mask.keys())[0]
            mask_img = Image.open(mask_name)
        else:
            print("No mask uploaded; defaulting to black mask (no change).")

    # ----- dispatch to the chosen task -----
    if task == "Denoising":
        noisy = add_gaussian_noise(input_img, sigma=35.0)
        out = denoise_image(noisy)
        show_side_by_side(noisy, out, "Noisy input", "Denoised output")

    elif task == "Super-Resolution":
        out = super_resolve(input_img)
        show_side_by_side(input_img, out, "Original (low-res)", "Super-resolved")

    elif task == "Colorization":
        gray = to_grayscale(input_img)
        out = colorize_image(gray)
        show_side_by_side(gray, out, "Grayscale input", "Colorized output")

    elif task == "Inpainting":
        if mask_img is None:
            # create empty mask if none given
            mask_img = Image.new("L", input_img.size, color=0)
        out = inpaint_image(input_img, mask_img)
        show_side_by_side(input_img, out, "Original", "Inpainted output")

    else:
        print("Unknown task:", task)

# 1. Denoising demo
run_task_on_uploaded_image(task="Denoising", use_mask=False)

# 2. Super-resolution demo
run_task_on_uploaded_image(task="Super-Resolution", use_mask=False)

# 3. Colorization demo
run_task_on_uploaded_image(task="Colorization", use_mask=False)

# 4. Inpainting demo (will ask for both image and mask)
run_task_on_uploaded_image(task="Inpainting", use_mask=True)

!pip install -q scikit-image datasets

!pip install -q scikit-image datasets

from datasets import load_dataset
from PIL import Image
import numpy as np
import os
from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim
import cv2
import glob

# ==========================================
# Step 1: Download a small open dataset
#         Using 'beans' (leaf images, 3 classes)
# ==========================================

beans = load_dataset("beans", split="validation")  # ~128 images[web:102]

os.makedirs("test_clean", exist_ok=True)
os.makedirs("test_noisy", exist_ok=True)
os.makedirs("test_lowres", exist_ok=True)

def center_resize(pil_img, size=256):
    w, h = pil_img.size
    min_side = min(w, h)
    left = (w - min_side) // 2
    top = (h - min_side) // 2
    right = left + min_side
    bottom = top + min_side
    cropped = pil_img.crop((left, top, right, bottom))
    return cropped.resize((size, size), Image.BICUBIC)

for i, sample in enumerate(beans):
    img = sample["image"].convert("RGB")
    img = center_resize(img, size=256)

    clean_path = f"test_clean/img_{i:03d}.png"
    noisy_path = f"test_noisy/img_{i:03d}.png"
    lowres_path = f"test_lowres/img_{i:03d}.png"

    img.save(clean_path)

    # Noisy version for denoising
    noisy = add_gaussian_noise(img, sigma=25.0)
    noisy.save(noisy_path)

    # Low-res version for super-resolution (4x downscale)
    lr = img.resize((64, 64), Image.BICUBIC)
    lr.save(lowres_path)

print("Saved clean, noisy, and low-res test images (beans dataset).")

# ==========================================
# Step 2: Helper functions for PSNR & SSIM
# ==========================================

def load_image_pairs(folder_clean, folder_degraded, ext="png"):
    clean_paths = sorted(glob.glob(os.path.join(folder_clean, f"*.{ext}")))
    deg_paths = sorted(glob.glob(os.path.join(folder_degraded, f"*.{ext}")))
    pairs = []
    for cp, dp in zip(clean_paths, deg_paths):
        clean = Image.open(cp).convert("RGB")
        deg = Image.open(dp).convert("RGB")
        pairs.append((clean, deg))
    return pairs

def compute_metrics(clean_img: Image.Image, pred_img: Image.Image):
    clean_np = np.array(clean_img)
    pred_np = np.array(pred_img)

    if clean_np.shape != pred_np.shape:
        pred_np = cv2.resize(pred_np, (clean_np.shape[1], clean_np.shape[0]))

    p = psnr(clean_np, pred_np, data_range=255)
    s = ssim(clean_np, pred_np, channel_axis=-1, data_range=255)
    return p, s

# ==========================================
# Step 3: Quantitative evaluation – Denoising
# ==========================================

denoise_pairs = load_image_pairs("test_clean", "test_noisy", ext="png")

psnr_dn, ssim_dn = [], []
os.makedirs("denoised_outputs", exist_ok=True)

for idx, (clean_img, noisy_img) in enumerate(denoise_pairs):
    pred = denoise_image(noisy_img)  # your existing function
    pred.save(f"denoised_outputs/denoised_{idx:03d}.png")

    p, s = compute_metrics(clean_img, pred)
    psnr_dn.append(p)
    ssim_dn.append(s)

    if idx < 3:
        print(f"[Denoise] Sample {idx}: PSNR={p:.2f}, SSIM={s:.3f}")

print("\n=== Denoising (Beans) ===")
print(f"Average PSNR: {np.mean(psnr_dn):.2f} dB")
print(f"Average SSIM: {np.mean(ssim_dn):.3f}")

print("\n=== Denoising (Beans) ===")
print(f"Average PSNR: {np.mean(psnr_dn):.2f} dB")
print(f"Average SSIM: {np.mean(ssim_dn):.3f}")

# ==========================================
# Step 4: Quantitative evaluation – Super-Resolution
# ==========================================

sr_pairs = load_image_pairs("test_clean", "test_lowres", ext="png")

psnr_sr, ssim_sr = [], []
os.makedirs("sr_outputs", exist_ok=True)

for idx, (clean_img, lowres_img) in enumerate(sr_pairs):
    pred = super_resolve(lowres_img)  # your existing function
    pred.save(f"sr_outputs/sr_{idx:03d}.png")

    p, s = compute_metrics(clean_img, pred)
    psnr_sr.append(p)
    ssim_sr.append(s)

    if idx < 3:
        print(f"[SR] Sample {idx}: PSNR={p:.2f}, SSIM={s:.3f}")

print("\n=== Super-Resolution (Beans) ===")
print(f"Average PSNR: {np.mean(psnr_sr):.2f} dB")
print(f"Average SSIM: {np.mean(ssim_sr):.3f}")

print("\n=== Super-Resolution (Beans) ===")
print(f"Average PSNR: {np.mean(psnr_sr):.2f} dB")
print(f"Average SSIM: {np.mean(ssim_sr):.3f}")

import pandas as pd

results = {
    "task": ["denoising", "super_resolution"],
    "avg_psnr": [float(np.mean(psnr_dn)), float(np.mean(psnr_sr))],
    "avg_ssim": [float(np.mean(ssim_dn)), float(np.mean(ssim_sr))],
}

df_res = pd.DataFrame(results)
df_res.to_csv("quantitative_results_beans.csv", index=False)
df_res

"""These numbers show that the current system performs poorly on denoising and moderately on super‑resolution, relative to typical image‑restoration benchmarks.

Denoising results
PSNR = 10.2 dB and SSIM = 0.074 are very low.


Values around 10 dB and SSIM near 0 indicate that the restored images are still very far from the clean reference and structurally quite different.

Interpretation:

The diffusion‑based “denoiser” is not reconstructing the same image, but rather generating a new plausible image, so pixel‑wise metrics punish it heavily.

The denoising module noticeably changes the image appearance but fails to recover the original content as measured by PSNR/SSIM, even if it may look visually nicer in some cases.

Super‑resolution results

PSNR = 19.7 dB and SSIM = 0.40 are better but still below what is expected from a strong super‑resolution model on aligned data.

Interpretation:

The super‑resolution module is partially successful: it adds detail and improves resolution compared to the low‑res input, but the reconstructed images still differ significantly from the ground truth at the pixel and structural level.

This likely reflects the generative nature of the upscaler (it hallucinates textures rather than exactly reversing downsampling), leading to reasonable visual quality but only moderate quantitative scores.
"""

